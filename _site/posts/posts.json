[
  {
    "path": "posts/2023-04-23-japonca/",
    "title": "Japonca",
    "description": "Japonca yolculuğuna giriş",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2023-04-23",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nGiriş\r\nNeden Japonca?\r\nBu zamana kadar neler öğrendim?\r\nPlanım ne?\r\nNasıl gidiyor?\r\n\r\nGiriş\r\nYıllarca o ya da bu sebepten askıya alınmış onca Japonca öğrenme hevesinden sonra, şimdi Japonca’ya gerçekten zaman ayırabilirmişim gibi hissediyorum. Hazır bunu hissetmişken, dedim ki bu süreci biraz da kayıt altına alayım. Belki ileride faydalı bir materyale dönüşür. Şu an ne yaptığıma gelmeden önce, cevaplamak istediğim iki önemli soru var. 1: “Neden Japonca?” 2: “Bu zamana kadar neler öğrendim?”. Öyleyse başlayalım!\r\nNeden Japonca?\r\nBir dil öğrenmenin en temel sorusu: “Neden?”. En eskiye gidersek, Japonca’nın ve Japonya’nın hayatıma girişini herhalde Barış Manço’nun Japonya konserine borçluyum: “Domates biber nassu nassu”dan “hahahaha ha hapşu, odaijini”ye çok enerjik olan bu konseri unutmak elde değil. Tabii bu sadece bir tanışmaydı; daha küçüktüm ve uzun bir süre boyunca da ingilizce dışında yabancı bir dil ilgimi çekmeyecekti. Benim hayatımın ingilizceye odaklı olduğu uzun dönem boyunca tabii ki Japonca hakkında yer yer bilgiler edinmiştim. Mesela, (çok da kabul görmediğini sonradan öğrensem de) Türkçe ve Japonca’nın aynı dil ailesinden geldiği bilgisi ya da Türkçe ve Japonca’nın dil bilgisel olarak birbirine benzediği bilgisi. Bu gibi bilgilerin ışığında bir gün mutlaka Japonca öğrenmek istiyordum ama o zamanlar daha bunu nasıl yapacağım hakkında çok bir fikrim yoktu ve yapılacaklar listemde bir kırıntı olarak kaldı uzun bir süre bu düşünce.\r\nBu düşünce kırıntıları üniversite yıllarımın son yıllarına kadar o listede kaldı. Sonra önce anime mi girdi yoksa hayatıma yoksa Japonca mı emin değilim ama ben Japonca’ya sanırım ilk kez ciddi olarak arkadaşım Klausvonsviç’le birbirimizi indüklememiz sonucunda girdim. Anime izlemeye başlamak Japonca öğrenmeye olan isteğimi oldukça arttırdı tabii ki. Bunun yanında yine o dönemlerde dinlemeye başladığım Japon müzikleri de oldukça ilgimi çekiyordu. Fusion, funk, jazz, rock ve pop gibi tarzlarda çok başarılı müzikler yaptıklarını da keşfettikten sonra artık oldukça fazla sebebim olmuştu bu dili öğrenmek için.\r\nBunlara ek olarak, Japonca’nın tamamıyla farklı bir(3) alfabeye sahip olması da ilginçti benim için. Tek bir kelimeyle söylemem gerekirse havalıydı.\r\nÖzetle neden Japonca sorumun cevabı böyle.\r\nBu bölümü kapamadan önce “Fast and Furious: Tokyo Drift” filmine ve Kesmeşeker’in “Japonca” adlı şarkısına da teşekkürlerimi sunmak isterim.\r\nBu zamana kadar neler öğrendim?\r\nJaponca’yı tekrar öğrenmeye başladığım şu noktada, sıfırdan başlamadığımı belirtmem lazım. Geçen yıllarda yer yer Japonca’yla olan istişarelerimizden biriken az çok bir deneyimim var. Sanırım ilk olarak alfabeyle başlamıştım. Hiragana ve katakanayı öğrenmiştim. Tabii ki ilk seferde oturmadı, ikinci seferde de oturmadı. Unuttum ve tekrar tekrar öğrendim. Arkadaşlarımın isimlerini yazarak pratik yaptığımı hatırlıyorum. Ayrıyetten basit bir manga indirip, anlamasam da diyalogları okumaya çalışmıştım, alfabeyi okumaya alışayım diye.\r\nBir nokta da yavaş yavaş birkaç kanji de ezberleyeyim diye düşünmüştüm. O yüzden bunun için kaynak arayıp birkaç kitap gözüme kestirmiştim. Bu kitapları takip ederek her gün, her hafta bir iki kelime ve kanjisini öğrensem sonra ileride işimi kolaylaştırır diye düşünmüştüm. Tabii teoride kulağa hoş gelse de pratikte oldukça sıkıcı olduğu için her seferinde başarısızlıkla sonuçlandı bu girişimim. Tabii kanjileri bir bağlam içinde öğrenmeye çalışıyordum ama daha ne kadar japonca biliyordum ki? O bağlamı kurmak için bulduğum cümleler de yeni oluyordu benim için ve bu işimi bayağı uzatıyordu. Neden böyle bir şey yapar ki insan kendisine, akıl alır gibi değil. Bu öğrenme sürecini; kelime ve cümle kalıplarını Anki’ye kaydedip tekrar ederek yapıyordum. Ayrıyetten Goldlist Metodu diye bir yöntemi de denemiştim. Şimdi dönüp baktığımda, sadece Hiragana ve Katakana pratiği açısından işime yaradı galiba bu yaptıklarım.\r\nDil bilgisi konusunda Erin diye bir siteyi takip etmeye çalışmıştım bir süre. Şimdi güncellemişler sanırım. Ama dil bilgisi için en sevdiğim kaynak Asialogy’nin Youtube’daki Japonca dersleriydi. Bir noktaya kadar takip edip bırakmıştım maalesef. Bunların dışında Genki kitabına başlamaya çalışmıştım ama pek de başlamıştım diyemem.\r\nGenel anlamda ise o kadar anime izlememin dinleme açısından bana bayağı faydasının olduğunu düşünüyorum. Özellikle, izlediğim şeyi bilinçli bir şekilde dinlememin. Bununla kast ettiğim, karakterlerin yer yer kullandığı kalıpları yakalamak. Ne gibi kalıpları ne gibi durumda söylediklerini duya duya bir kulak aşinalığı oluştu kesinlikle.\r\nÖzetle bu “laylaylom Japonca öğreniyorum arada sırada yey” yıllarının sonunda aklımda kalanlar şöyle:\r\nTozlu bir Hiragana ve Katakana.\r\nNeredeyse hiç kanji.\r\nAz ve tozlu bir dil bilgisi.\r\nOldukça etkili(tartışılır) bir anime kelime haznesi ve kulak yatkınlığı.\r\nPlanım ne?\r\nBu sefer önceden yaptığım şeylerden ders çıkararak öncelikle kanji öğrenmeye çalışmayacağım aktif olarak. Fakat sıkça denk geldiğim ve yazımı kolay kanjiler olursa bunları öğrenmeyi düşünüyorum.\r\nAnki, goldlist gibi ezberleme uygulamalarından hep sıkılıyorum o yüzden onlarla şu noktada uğraşmayı düşünmüyorum.\r\nGenelde bir dil öğrenmeye ilk başlarken bir dil bilgisi egzersiz kitabını takip etmeyi seviyorum. O bilinmez uçsuz bucaksız dil denizinde sakin bir liman oluyor benim için dil bilgisi. Tabii bu limanda sonsuza kadar kalmayı düşünmüyorum. Genki’nin Japonca serisi bayağı ünlü olduğu için ondaki egzersizleri yapmayı düşünüyorum. Fakat Genki’den önce Asialogy’nin Japonca serisini sonunda baştan sona izlemek istiyorum. Türkçe ile karşılaştırarak basit bir şekilde anlattığı için çok önemli bir kaynak olduğunu düşünüyorum. Japonca’da şöyle beni neler beklediği konusunda genel bir fikir edinmek için çok faydalı olur bence.\r\nNasıl gidiyor?\r\nYukarıda da bahsettiğim gibi şimdilik Asialogy’nin videolarına odaklandım. Fakat videoları sadece izleyip geçmiyorum, aynı zamanda notlar da alıyorum. Özetle; bir bölümü izliyorum, izlerken Japonca için ayırdığım bir deftere de dil bilgisi notlarını ve bazı örnek cümleleri yazıyorum. Bu noktada amacım her konuyu derinlemesine oturtmak değil, sadece Japonca’yla tanışmak. Bu yüzden videolarda çok vakit kaybetmiyorum. Gerekli notlarımı alıyorum ve ilerliyorum. İleride egzersizler yaptıkça bu notlarıma danışmanın faydalı olacağını düşünüyorum.\r\nŞu an videoların yarısından çoğu bitti. İlk başlarda daha motive bir şekilde her gün izliyordum ama bi süre sonra tempom düştü, şu an daha yavaş yavaş ilerliyorum. Ama ilerliyorum, önemli olan da bu.\r\nOnun dışında bir defter tuttuğumdan bahsettim. Bir dil öğrenirken defter tutmanın çok faydalı olduğunu düşünüyorum, özellikle başlangıç aşamalarında. İnsana bir referans noktası veriyor bence ve amaçsızca dil içinde kaybolmasının önüne geçiyor. Benim kişisel deneyimim, dilde ilerledikçe defterden uzaklaşmam şeklindeydi. İtalyanca ve Almanca için bayağıdır bir defter tutmuyordum mesela, fakat geçen aylarda öğrenmek istediğim kelimeleri bir deftere yazmaya başladım. O defteri otobüste giderken açıp notlarımı tekrar etmek oldukça yardımcı oldu. Mutlaka öğrendiğiniz dil için ayrı bir defteriniz olsun.\r\nŞimdilik durum bu. İleride gelişmeler oldukça buraya yazacağım. . .\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-05-18T21:26:55+02:00",
    "input_file": "japonca.knit.md"
  },
  {
    "path": "posts/2021-12-18-confidence-intervals/",
    "title": "Confidence Intervals",
    "description": "Importance of reporting confidence intervals and effect size. Why and how confidence intervals relate to the P values.",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2022-12-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nProblem with p-values\r\nConfidence Intervals and p-values\r\n\r\nProblem with p-values\r\nP-values are informative but on its own it can be ambiguous. With enough sample size one can detect even the smallest difference and can say that the observed difference is significant. But is a minuscule difference relevant or interesting? Thus often it is actually more interesting to focus on the effect size (or maybe biological relevance). The degree of the observed difference.\r\nAnother thing one can look into is confidence intervals, which can inform us about the effect size and the uncertainty of our estimate.\r\nFirst we can calculate a confidence interval for a population mean estimate and then we can focus on the calculation of a confidence interval for difference of 2 samples.\r\nFrom a population with a mean \\(\\mu_{pop}\\) we can take a sample of 30 (N = 30). With N of 30 we can apply CLT, that states that mean of the sampling \\(\\bar{X}\\) should follow a normal distribution with the mean of \\(\\mu_{pop}\\) and with standard error of around \\(\\frac{sd_X}{\\sqrt{N}}\\).\r\nOur mean estimate of the population is a random variable and by using a confidence interval we can indicate the variability in our estimate.\r\nSo lets calculate the 95% confidence interval (95% CI) for our estimate. 95% CI would tell us that with probability of 95% our interval would fall on the true population parameter that we are estimating. (Note that it does not mean the opposite, that true value would fall in this interval 95% of the time.)\r\nSince we used CLT, we are dealing with a normal distribution. Specifically a standard normal distribution as given by the following formula: \\(\\frac{\\bar{X} - \\mu_{pop}}{\\frac{sd_X}{\\sqrt{N}}}\\) (which was discussed in previous posts). Standard normal distribution is a normal distribution with mean = 0 and sd = 1. If we take the 95% of the area under this normal distribution we would see that it would encapsulate the area starting from -1.96 to 1.96.\r\n\\[\r\n-1.96 < \\frac{\\bar{X} - \\mu_{pop}}{\\frac{sd_X}{\\sqrt{N}}} < 1.96\r\n\\]\r\nSo probability of our estimate to fall in these regions is 95%. If we arrange it further we would get:\r\n\\[\r\n\\bar{X} - \\frac{1.96sd_X}{\\sqrt{N}} < \\mu_{pop} < \\bar{X} + \\frac{1.96sd_X}{\\sqrt{N}}\r\n\\]\r\nSo as the definition stated: With probability of 95%, our interval would include the true population parameter that we are estimating. In other words, the population parameter is a fixed value and the range of the CI is defined by random variables. This defined range covers the population parameter 95% of the time.\r\n\r\n\r\n\r\nFigure 1: Confidence Interval\r\n\r\n\r\n\r\nIf we had used the CLT with low sample size instead of 30 for example, then CLT would not be accurate since our samplings would deviate from normal distribution and our estimate of 95% CI would include the population parameter less than 95% of the time. In this case we would normally use t-test. T distribution has larger tails so using it will give us a bigger 95% CI, because now the null distribution will cover a larger area to encapsulate the 95% of the area. So in the end if we were to calculate the 95% area according to the t distribution we would get an accurate 95% CI.\r\nConfidence Intervals and p-values\r\nLet’s say you are interested in two mean differences. \\(\\bar{X} - \\bar{Y}\\). Let’s call this difference d. Suppose we calculate a 95% CI for this difference d according to the t-distribution and we get:\r\n\\[\r\nd - \\frac{2sd_d}{\\sqrt{N}} < 0 < d + \\frac{2sd_d}{\\sqrt{N}}\r\n\\]\r\nIf this interval fails to include the the 0, then it would mean either the lower bound is bigger than 0 or the upper bound is smaller than 0. \\(d - \\frac{2sd_X}{\\sqrt{N}} > 0\\) and \\(d + \\frac{2sd_X}{\\sqrt{N}} < 0\\) respectively. If we arrange the equation, this would also mean \\(\\frac{d}{\\frac{sd_d}{\\sqrt{N}}} > 2\\) or \\(\\frac{d}{\\frac{sd_d}{\\sqrt{N}}} < 2\\). Considering that \\(d = \\bar{X} - \\bar{Y}\\), \\(\\frac{\\bar{X} - \\bar{Y}}{\\frac{sd_d}{\\sqrt{N}}}\\) is the t-test statistic (t value). Coincidentally when the t-test statistic is bigger than 2 (arbitrary here) it also means \\(p < 0.05\\). Likewise if we calculated 99% CI, we would get a result like: \\(t > constant\\) where a t value bigger than this constant would indicate \\(p < 0.01\\) in a t value table.\r\nIn the next post I will look into power calculations.\r\nFun fact: I started writing this post exactly one year ago and then I stopped. I looked at it and finished only now…\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-18-confidence-intervals/images/ci.png",
    "last_modified": "2022-12-17T11:54:13+01:00",
    "input_file": "confidence-intervals.knit.md",
    "preview_width": 1061,
    "preview_height": 677
  },
  {
    "path": "posts/2021-11-27-inference/",
    "title": "Random variable Characteristics, Statistical Inference & t-test",
    "description": "Here I talk about some characteristics of random variables, then we see how these characteristics are applied in statistical testing and t-test makes an appearance.",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nMath with random variables\r\nExample for sum of variances\r\n\r\nIn practice\r\nIllustration: sample size vs normality\r\n\r\n\r\n\r\n\r\nMath with random variables\r\nIt is time we talk about some math(dört işlem) with random variables. As CLT indicates (see the post), we know that when sample size is large the means are distributed randomly. To illustrate the math with random variables more easily, let’s consider we have the following measurements and let’s assume that they are normally distributed: \\(X = (10cm, 20cm, 30cm)\\). It has a mean \\(\\bar{X}\\) of 20 and sd \\(S_x\\) of ~8.2.\r\nIf we add or subtract a constant value from our random variable, the new mean \\(\\bar{X}{new}\\) becomes \\(\\bar{X} + constant\\) and our sd \\(S_x\\) does not change since the average distance of the variables to the mean is still the same.\r\nIf we multiply our random variable \\(X\\) with a constant, for example we converted our cm measurements to mm, the new mean \\(\\bar{X}{new}\\) becomes \\(\\bar{X} * constant\\) and our sd \\(S_x\\) becomes also \\(S_{new} = S_x * constant\\) (now the spread of our data points increased).\r\n\r\n\r\n\r\nFigure 1: Some math with random variables\r\n\r\n\r\n\r\nSo far it is clear I believe. So what if we sum or multiply our random variable \\(X\\) with another random variable \\(Y\\). This type of a calculation we encounter more. Generally we are interested in mean differences of two populations for example. Then we subtract these means right? Please note that, as our random variables, their mean and sd are also random variables. So let’s see how the things will work out!\r\nThe mean:\r\nAs previously, when we sum or subtract the two random variables their mean also gets summed up or subtracted. So: \\(mean(X + Y) = \\bar{X} + \\bar{Y}\\) or \\(mean(X - Y) = \\bar{X} - \\bar{Y}\\). This was easy.\r\nThe Sd:\r\nThis time we need to assume a few things and we will also work with variance. Variance is square of the sd \\(S_x^2\\). The reason we will use variance instead of sd is that simply it makes the math much more simple.\r\nIf we skip the math we get this equation:\r\n\\[\r\nVar(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)\r\n\\]\r\nHere we see a new fellow: Covariance (but we will get rid of him in a sec şşşşşş). Which is:\r\n\\[\r\nCov(X,Y) = Cor(X,Y)*\\sigma_X*\\sigma_Y\r\n\\]\r\nHere we see a more familiar face Correlation. It has come to tell us our first assumption, our random variables should not correlate with each other. Which means \\(Cor(X,Y) = 0\\), which makes \\(Cov(X,Y) = 0\\), which results in:\r\n\\[\r\nVar(X+Y) = Var(X) + Var(Y)\r\n\\] This tells us that variance of the sum of \\(X\\) and \\(Y\\) is equal to sum of their variances. GIVEN they are not correlated.\r\nExample for sum of variances\r\n\r\n\r\nset.seed(1)\r\n\r\n# Almost not correlated random variables:\r\nx <- runif(n = 100, min = 10,max = 50)\r\ny <- runif(n = 100, min = 60,max = 100)\r\n\r\n\r\n# Correlated random variables:\r\na <- 1:100\r\nb <- seq(50,200, length.out=100)\r\n\r\n\r\n\r\nWhen random variables are not correlated:\r\nCor(X,Y) = 0.02\r\nalmost no correlation.\r\nvar(x) + var(y) = 232.78\r\nvar(x + y) = 236.74\r\nclose enough!\r\nWhen random variables are correlated:\r\nCor(a,b) = 1\r\nfull correlation\r\nvar(a) + var(b) = 2773.87\r\nvar(a + b) = 5324.37\r\nAs you can see when our random variables were correlated our variance estimate was way off.\r\nIn practice\r\nSo in practice we compare the mean differences to a distribution. In the first postthe first post we had calculated the null distribution by taking unreal amount of samples from the population. Now we will do it using statistics.\r\nTo calculate a test statistic we divide the random variable with its standard deviation. This, in theory gives us a normal distribution with mean = 0 and sd = 11 (given high sample size)2. This also allows us to standardize the data. Even if the original data values are at the range of thousands, we standardize it to a standard distribution.\r\n\\[\r\n\\frac{\\bar{X}-\\bar{Y}}{sd_{XY}}\r\n\\]\r\nHere note that both numerator and denominator are random variables.\r\nAs we have seen above if we subtract two random variables, their mean also gets subtracted, hence numerator. (we are using sample means as an estimate of the real population means.)\r\nWe also saw that when we subtracted two random variables the new variance is equal to the sum of the individual variances. Thus \\(sd_{XY}\\) will be equal to \\(\\sqrt{\\frac{s_X^2}{M} + \\frac{s_Y^2}{N}}\\) where M and N are sample sizes of random variables X and Y.\r\nIf sample size is high, according to CLT our random variables will be also normal. Thus the parameters at the numerator and denominator will be also normal3.\r\nThe issue is division of two normal values may not be normal. This is due to the fact that we can underestimate the sd4 (Especially when the sample size is low). Thus our probability of observing bigger test statistic values increases. (see the Illustration section at the bottom)\r\nThis change in probabilities creates a new distribution, that we know as Student’s t distribution. More on the story\r\nThis is the final version of our statistic. Mean difference of the random variables divided by its standard deviation5.6\r\n\\[\r\n\\frac{\\bar{X}-\\bar{Y}}{\\sqrt{\\frac{s_X^2}{M} + \\frac{s_Y^2}{N}}}\r\n\\]\r\nWilliam Sealy Gosset\r\n\r\n\r\n\r\nFigure 2: https://en.wikipedia.org/wiki/William_Sealy_Gosset\r\n\r\n\r\n\r\nCLT is dependent on high sample size. Our estimate of variances are accurate when the sample size is high and thus the test statistic follows a standard normal distribution. However it does not account for the underestimation of variance when our sample size is not that high. t-distribution looks similar to normal distribution formula but it also comes with an additional flavor called degrees of freedom (df), that accounts for this.\r\n\r\n\r\n\r\nFigure 3: t-distribution with different degrees of freedom, code from: https://stackoverflow.com/questions/49236741/plot-student-s-t-distribution-with-degrees-of-freedom\r\n\r\n\r\n\r\nFor the small sample sizes, distribution becomes more wide(higher tails, red line) and it accounts for the higher probability of observing extremes7. As the sample size increases t-distribution also starts to resemble the normal distribution.\r\nIllustration: sample size vs normality\r\nHere I will directly use the code from the book “Data Analysis for the Life Sciences” to show it.[1] Here we have a population of mice, half treated with a control diet other half treated with different diet and we compare their weights. From each group we take samples and calculate the mean difference of our samples. We repeat this 10000 times. We will compare different sample sizes: 3, 12, 25 and 50.\r\n\r\n[1] 2.375517\r\n\r\n\r\nFigure 4: Mean differences\r\n\r\n\r\n\r\nHere we see with quantile plots, how our mean differences compare to a normal distribution.\\(\\bar{X}-\\bar{Y}\\) We can see that even at sample size of 3 our differences are relatively normal. This is mainly due to the fact that weight is a relatively normally distributed factor. Since the population is normal our samples are also normal. Thus we see a good fit even with low sample size.\r\nHowever as I talked about it above, we also divide mean difference of random variables with its sd. \\(\\frac{\\bar{X}-\\bar{Y}}{\\sqrt{\\frac{s_X^2}{M} + \\frac{s_Y^2}{N}}}\\) For the next plot, we divide mean differences with its sd. We repeat this 10000 times. Now we see that ratio of these two random variables indeed deviate from normal at normal distribution at low sample size. At even sample size of 12 we see some deviation at the edges. At the higher sample sizes it again fit to normal distribution.\r\n\r\n\r\n\r\nFigure 5: Mean differences divided by standard deviations\r\n\r\n\r\n\r\n\r\n\r\n\r\n1. Irizarry, R. A., & Love, M. I. (2015). Data analysis for the life sciences. https://leanpub.com/dataanalysisforthelifesciences\r\n\r\n\r\n(standard normal distribution)↩︎\r\nCLT↩︎\r\nIn the first post when we plotted the mean differences we saw that they were normally distributed↩︎\r\n(denominator)↩︎\r\naka SEM↩︎\r\nSince in this example we are comparing two random variables, if we do a t-test it would be called two sample t-test↩︎\r\nProbability of observing extremes increases, because we tend to underestimate the true variance at low sample size, as I discussed above↩︎\r\n",
    "preview": "posts/2021-11-27-inference/inference_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-12-05T13:25:15+01:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-11-20-random-variables-and-distributions/",
    "title": "Random Variables, Distributions and CLT",
    "description": "We talk about CLT, sample size and SEM",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nEditor’s Notes / Corrections (05.12.21)\r\nIntro\r\nCentral Limit Theorem\r\nWhat is SEM actually?\r\nSimulation with different sample sizes\r\nSample size vs Replicates\r\nHow to calculate SEM in practice?\r\n\r\n\r\n\r\n\r\n\r\nEditor’s Notes / Corrections (05.12.21)\r\nIn the last paragraph of this post I discuss calculation of SEM with replicates and without replicates and I do some illustrations at the figures 6 and 7. I think they are conceptually correct but there are a few points that I must correct for.\r\nAs I was trying to plot the flow cytometry data this weekend, I found myself thinking about sample size and replicates again. Ideally you can take mice and divide them into two groups like: control and treatment. Then each mouse is relatively random sampled. (Although they are technically inbred.). In my experiment, I divide cells from one mice into groups. Then I started thinking if comparing these cell groups with each other made sense, since technically they come from same mouse. I thought they are also kind of dependent. But since we divide the cells randomly into groups, perhaps they are actually randomly sampled. But then again I do the same experiment couple of times and then I have data from other mice, which allows me to make broader conclusions. I guess also in cell culture experiments people don’t take a new batch for each experimental condition. They divide the batch into groups and then repeat the experiment with another batch. Then compare the results from different batches. That is/was a bit confusing. There are different depths in an experiment, but I guess everything depends on the question you are asking. If I am interested in mouse weight, then each mouse is my sample or actually replicate. If I am interested in the cells then each experimental group with the cells becomes my sample. (I am not sure but maybe the most ideal would be treating cells from each mice with just one treatment, so that each cell population are independent from each other (like new batch of cells for each treatment). But then it would result in animal massacre and that does not make sense at all.)\r\nAnyway in the figures 6 and 7, I say that if I take 4 mice and calculate the mean of the parameter of interest and repeat this 3 times, then the sd of these means are actually SEM estimation from single sampling. In other words if I measured the weights from 4 different mice, I can calculate the SEM via dividing sd of my measurements to square root of 4(n). If I repeated this experiment many more times, sd of the mean of each repeat would be the SEM I calculated earlier. The problem is, below I claimed that if we have replicates we can calculate the SEM by directly taking the sd of the replicate means, we don’t need to estimate it since we have replicates. But I was confused about the term of “biological replicates”. In theory if you are interested in the mean weight differences in different experimental conditions, each mice you use is a biological replicate. So when you compare 12 control mice to 12 treated mice you have sample size of 12. One would not repeat this experiment with more groups of 24 mice to get a mean from each of them to compare the effect of control vs treatment conditions. A different lab may repeat this experiment with 24 mice and then comparing these two experiments would be a question of reproduciblity (like meta analysis). In that case we would have sample size of 2 I think, and we would go one step deeper. So in reality we never calculate SEM from replicate standard deviations. We don’t need to. If we had 12 mice we wouldn’t use 6 of them to do the experiment and then use the other 6 to repeat it, we would use all 12 mice and keep our sample size high.\r\nI also wanted to compare different ways of estimating SEM as I write this. I took samples with the size of (n=3, 10, 30) from a population (σ = 20, size = 100000). Then I calculated the sd of sampling means(SEM) with 3 different methods:\r\n\\(\\frac{\\sigma}{\\sqrt{n}}\\)\r\nI used the population sd(σ). Which is indicated as horizontal line. (Expectation)\r\n\\(\\frac{sd}{\\sqrt{n}}\\)\r\nI used sd of the sample as the estimate of σ. Purple dots\r\nsd of the sampling means\r\nI repeated the sampling process 1000 times and calculated the SEM directly via taking sd of the sampling means. Orange dots\r\nThe whole process was repeated 25 times showing the results I got each 25 times.\r\n\r\n\r\n\r\nFigure 1: Different sample sizes and estimation of SEM via different methods\r\n\r\n\r\n\r\nOne can see that estimating SEM via \\(\\frac{sd}{\\sqrt{n}}\\) is not actually that far away from the expectation especially with increasing sample size. When we do replicates it looks more accurate true, but when I did just 3 replicates that estimate was also way off(not shown). It is fun little graph. I hope I was able to explain it.\r\nIntro\r\nIn the last post we bought thousands of mice to get the null distribution of the mouse weights. But we cannot do that every time can we? At this point statistical inference comes to the rescue, which helps us to infer the probability of observing certain outcome, with just from small amount of samples. It can do its magic because sometimes the values we have can be approximated with a certain distribution. Our mean weight differences, when plotted follows a normal distribution for example (red line). Thus if such a distribution is applicable to our values one can use that distribution to get an approximate probability of certain outcome.\r\n\r\n\r\n\r\nFigure 2: Null distribution from last post\r\n\r\n\r\n\r\nOk, let’s use normal distribution, but how can we adjust this distribution to fit our data? In order to define a normal distribution we need two values: mean(μ) and standard deviation(σ) of our population. (as one can see from the scary normal distribution formula)\r\n\\[\r\n\\mbox{f}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left( \\frac{-(x-\\mu)^2}{2 \\sigma^2} \\right)}\r\n\\]\r\nCentral Limit Theorem\r\n\r\n\r\n\r\nFigure 3: Population and Sample Parameters\r\n\r\n\r\n\r\nSo we need to estimate our population mean \\(μ_X\\) and population sd \\(\\sigma_X\\).\r\nwhich we can do with our sample mean \\(\\bar{X}\\) and sample sd \\(s_X\\), with the help of one and only Central Limit Theorem(CLT).\r\nCLT says that when the sample size (N) is large, the average of these random samples \\(\\bar{X}\\) follow a normal distribution that has the mean of the population \\(μ_X\\) and has a sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\).\r\nIn another words, imagine that we sample 30 people randomly and record the mean height. Then repeat this couple of times so that we have several means. Distribution of these sampling means would follow a normal distribution with the mean \\(μ_X\\) and with sd \\(\\frac{\\sigma_X}{\\sqrt{30}}\\).\r\nNote that \\(\\frac{\\sigma_X}{\\sqrt{N}}\\) is also famously known as Standard Error of the Mean (SEM).\r\nWhat is SEM actually?\r\nStandard Error of the Mean.. What a confusing name… But apparently this was not the case before. This is actually its nick name it seems. Its full name is “The estimated standard deviation of the sampling distribution of x-bar” according to this discussion. Which is longer but more explanatory. As it suggests, SEM is just the standard deviation of the means(\\(\\bar{X}\\)) of your samples. So you took four measurements and calculated mean of it, repeated this 10 times. You get 10 means, standard deviation of these means is SEM.\r\nOna sadece iş arkadaşları SEM der.\r\n\r\n\r\n\r\nFigure 4: Figure from: https://www.haberler.com/fotogaleri/yilan-hikayesi-nin-memoli-sinin-son-hali/\r\n\r\n\r\n\r\nSimulation with different sample sizes\r\nHere I simulated different populations with different characteristics, each with size of 200.000, mean of μ = ~130 and sd of σ = ~34. On the left we see the different populations (bimodal, normal, uniform). I sampled from these populations with different sample sizes (n=2, 10, 30, 50), calculated the mean and repeated this process 100 times (green histograms). I also plotted two normal distributions and means. Black lines show the calculations done by using population parameters; normal distribution with sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\) and mean: \\(μ_P\\). Red lines show the empirical estimate from our sampling means; normal distribution with sd \\(s_X\\) and mean \\(\\bar{X}\\).\r\nCLT\r\nNe olursan ol yine gel\r\ndiyen Mevlana,\r\nBen insanın değerini bölemem\r\nDoğu-batı, gavur-müslüm bir bana!\r\ndiyen Aşık Mahzuni Şerif gibi.\r\nHere one can see that actually even with sample size of 2 we get a quite accurate estimate of the mean and sd of the means(SEM). Although the actual mean calculations(histograms) are not completely normally distributed, it still resembles a normal distribution. As we increase the sample size we see that the distribution of the means get much more similar to the suggested normal distribution by CLT (with the mean of the population, and sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\)).\r\nAnother thing that we can notice is that SEM decreases as we increase the sample size. We get much more accurate estimate of the real population mean with high N.\r\nLastly we have seen that CLT did not care about the population distribution too much. In the end we got normally distributed means.\r\n\r\n\r\n\r\nFigure 5: Sample size differences illustration\r\n\r\n\r\n\r\nI would like to touch on a few points which always confused me. I think I finally got it. :)\r\nSample size vs Replicates\r\nI think sometimes it gets quite confusing to understand what the sample size is, at least for me.. If we look at the following figure, on the left side I took 4 samples. Let’s say I took 4 mice and measured their weight and that is it. In this case I have a sample size of 4 and I only performed 1 trial/replicate. On the right side I took 4 mice again but repeated this 5 times in total. Here my sample size is 4 again and I just replicated it 5 times.\r\nOn the right bottom I also illustrated that as we do more replicates we would expect these sampling means from each replicate to form a normal distribution. Higher our sample size is, more it resembles a normal distribution according to CLT. In statistics they say minimum sample size should be 30 as a rule of thumb, but as we also seen in the simulation, sample size of 10 was also reasonably good. As far as I read this rule of thumb depends mostly on the distribution of the population. Some confusing addition to this topic: Furthermore, as we do a statistic test we divide mean differences with estimated standard deviation. With this additional division, sample size starts to matter more apparently[1]. See the next post about this.\r\n\r\n\r\n\r\nFigure 6: Sampling\r\n\r\n\r\n\r\nHow to calculate SEM in practice?\r\nThe thing that confused me a lot is how to calculate the SEM in practice, especially when you have or don’t have replicates. In the next figure, on the left we measured weights of 4 mice again. According to \\(SEM = \\frac{\\sigma_X}{\\sqrt{N}}\\) we can get SEM. But since we don’t have data from billions of mice that live in the world, we don’t know the population standard deviation (σ). Good thing is we have sample sd (\\(s_X\\)) that we can use as estimate of σ. Sample standard deviation is calculated a little bit differently than population sd:\\[s_X = \\sqrt{\\frac{(X_i - \\bar{X})^2 }{N - 1}}\\]\r\nWe use sample mean and at denominator we have N-1 instead of N. This is simply because, when we estimate σ from sample sd we tend to underestimate it. Thus we boost it up by dividing with a smaller number. There is also a more logical explanations about this, which I can discuss later.\r\nSo we can calculate the \\(s_X\\) with this formula and calculate the estimate for SEM via \\(SEM = \\frac{s_X}{\\sqrt{N}}\\).\r\nTake the following paragraph and figure with caution. See the notes at the beginning of the post\r\nWhat if we have replicates? Then we already calculate mean for each replicate, thus we have more than one mean estimate. Thus one can directly calculate the sd of these means as usual to get the SEM. Which makes sense I think and also according to this resource. THUS, as far as I understood when you have replicates you don’t have to do this \\(\\frac{s_X}{\\sqrt{N}}\\) to get SEM (And I don’t know what to write to the place of \\(s_X\\) to be honest when you have replicates).\r\n\r\n\r\n\r\nFigure 7: Calculation of SEM\r\n\r\n\r\n\r\n\r\n\r\n\r\n1. Irizarry, R. A., & Love, M. I. (2015). Data analysis for the life sciences. https://leanpub.com/dataanalysisforthelifesciences\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-20-random-variables-and-distributions/images/Ps_mean.png",
    "last_modified": "2022-12-11T21:58:49+01:00",
    "input_file": "random-variables-and-distributions.knit.md",
    "preview_width": 1022,
    "preview_height": 781
  },
  {
    "path": "posts/2021-11-07-post/",
    "title": "Random Variables",
    "description": "first post",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nRandom Variables\r\nNull Distribution\r\n\r\n\r\n\r\n\r\nRandom Variables\r\n\r\n\r\n\r\nLet’s imagine we measured weight of 12 mice. Half treated with a special diet (treatment), other half with usual diet (control). We want to know whether this different diet has an effect on mouse weight or not.\r\n\r\n# A tibble: 12 x 2\r\n# Groups:   condition [2]\r\n   condition weight\r\n   <chr>      <dbl>\r\n 1 control     16.3\r\n 2 control     16.9\r\n 3 control     17.9\r\n 4 control     19.5\r\n 5 control     16.0\r\n 6 control     19.5\r\n 7 treatment   23.4\r\n 8 treatment   21.9\r\n 9 treatment   23.8\r\n10 treatment   22.5\r\n11 treatment   23.6\r\n12 treatment   25.0\r\n\r\nWe then calculate the mean of each group and look at the mean difference of weights.\r\n\\[\r\nμ_t - μ_c = 5.69 \r\n\\]\r\nResults indicate that treated mice are on average 32% heavier than the control ones. So why can’t we just leave it there?\r\nBecause the average values that we get are random variables. If we were to sample and measure another 12 mice for this experiment, we would get a different mean value. In fact these random variables are distribution of values. For example, if we took 10000 samples from the mouse population and measured the mean of these measurements, this would illustrate how this random variable is indeed random and that it varies.\r\n\r\n\r\n\r\nThus we can’t just say: ooh treated mice are 32%* heavier.. We need p-values, Confidence Intervals. There is a variability and we need to take this into account.\r\nNull Distribution\r\nLet’s order 20 mice and divide them randomly into groups of 10 and this time feed them with the same diet. Then again let’s calculate the mean difference of these groups and repeat this 10000 times. When we plot these mean differences we would get a distribution called Null distribution\r\n\r\n\r\n\r\nThis distribution basically shows the variability of random variables (in this case as mean difference), when there is actually no difference. Another thing it shows is that how probable it is to observe a certain value. In this case, how probable it is to observe a mean difference value of x when there is no difference in compared populations.\r\nSuppose that the mean difference of weight between treated mice and control mice was 1.55. If we look at our null distribution graph and calculate the percentage of the area that is higher than 1.55, we would see that it is approximately 6% of the plot. This means that, if there was no difference between groups; 6% of the time, we would get a value of 1.55 or higher. And this would be our p-value\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-07-post/post_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-11-14T22:42:51+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to bblog",
    "description": "hg",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-14T21:05:21+01:00",
    "input_file": {}
  }
]
