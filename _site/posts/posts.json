[
  {
    "path": "posts/2021-11-20-random-variables-and-distributions/",
    "title": "Random Variables, Distributions and CLT",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nCentral Limit Theorem\r\nWhat is SEM actually?\r\nSimulation with different sample sizes\r\nSample size vs Replicates\r\nHow to calculate SEM in practice?\r\n\r\n\r\n\r\n\r\n\r\nIn the last post we bought thousands of mice to get the null distribution of the mouse weights. But we cannot do that every time can we? At this point statistical inference comes to the rescue, which helps us to infer the probability of observing certain outcome, with just from small amount of samples. It can does its magic because sometimes the values we have can be approximated with a certain distribution. Our mean weight differences, when plotted follows a normal distribution for example (red line). Thus if such a distribution is applicable to our values one can use that distribution to get an approximate probability of certain outcome.\r\n\r\n\r\n\r\nOk, let’s use normal distribution, but how can we adjust this distribution to fit our data? In order to define a normal distribution we need two values: mean and standard deviation of our population. (as one can see from the scary normal distribution formula)\r\n\\[\r\n\\mbox{f}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left( \\frac{-(x-\\mu)^2}{2 \\sigma^2} \\right)}\r\n\\]\r\nCentral Limit Theorem\r\n\r\n\r\n\r\nSo we need to estimate our population mean \\(μ_X\\) and population sd \\(\\sigma_X\\).\r\nwhich we can do with our sample mean \\(\\bar{X}\\) and sample sd \\(s_X\\), with the help of one and only Central Limit Theorem(CLT).\r\nCLT says that when the sample size (N) is large, the average of these random samples \\(\\bar{X}\\) follow a normal distribution that has the mean of the population \\(μ_X\\) and has a sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\).\r\nIn another words, imagine that we sample 30 people randomly and record the mean height. Then repeat this couple of times so that we have several means. Distribution of these sampling means would follow a normal distribution with the mean \\(μ_X\\) and with sd \\(\\frac{\\sigma_X}{\\sqrt{100}}\\).\r\nNote that \\(\\frac{\\sigma_X}{\\sqrt{N}}\\) is also famously known as Standard Error of the Mean (SEM).\r\nWhat is SEM actually?\r\nStandard Error of the Mean.. What a confusing name… But apparently this was not the case before. This is actually it is nick name it seems. Its full name is “The estimated standard deviation of the sampling distribution of x-bar” according to this discussion. Which is longer but more explanatory. As it suggests, SEM is just the standard deviation of the means(\\(\\bar{X}\\)) of your samples. So you took four measurements and calculated mean of it, repeated this 10 times. You get 10 means, standard deviation of it is SEM.\r\nOna sadece iş arkadaşları SEM der.\r\n\r\n\r\n\r\nFigure 1: Figure from: https://www.haberler.com/fotogaleri/yilan-hikayesi-nin-memoli-sinin-son-hali/\r\n\r\n\r\n\r\nSimulation with different sample sizes\r\nHere I simulated different populations with different characteristics, each with size of 200.000, mean of μ = ~130 and sd of σ = ~34. On the left we see the different populations (bimodal, normal, uniform). I sampled from these populations with different sample sizes (n=2, 10, 30, 50), calculated the mean and repeated this process 100 times (green histograms). I also plotted two normal distributions and means. Black lines show the calculations done by using population parameters; normal distribution with sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\) and mean: \\(μ_P\\). Red lines show the empirical estimate from our sampling means; normal distribution with sd \\(s_X\\) and mean \\(\\bar{X}\\).\r\nHere one can see that actually even with sample size of 2 we get a quite accurate estimate of the mean and sd of the means(SEM). Although the actual mean calculations does not completely distributed normally, it still resembles a normal distribution. As we increase the sample size we see that the distribution of the means get much more similar to the suggested normal distribution by CLT (with the mean of the population, and sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\)).\r\nAnother thing that we can notice is that SEM decreases as we increase the sample size. We get much more accurate estimate of the real population mean with high N.\r\nLastly we have seen that CLT did not care about the population distribution too much.\r\n\r\n\r\n\r\nI would like to touch on a few points which always confused me. I think I finally got it.\r\nCLT\r\nNe olursan ol yine gel\r\ndiyen Mevlana,\r\nBen insanın değerini bölemem\r\nDoğu-batı, gavur-müslüm bir bana!\r\ndiyen Aşık Mahzuni Şerif gibi.\r\nSample size vs Replicates\r\nI think sometimes it gets quite confusing to understand what the sample size is, at least for me.. If we look at the following figure, on the left side I took 4 samples. Let’s say I took 4 mice and measured their weight and that is it. In this case I have a sample size of 4 and I only performed 1 trial/replicate. On the right side I took 4 mice again but repeated this 5 times in total. Here my sample size is 4 again and I just replicated it 5 times.\r\nOn the right bottom I also illustrated that as we do more replicates we would expect these sampling means from each replicate to form a normal distribution. Higher our sample size is, more it resembles a normal distribution according to CLT. In statistics they say minimum sample size should be 30 as a rule of thumb, but as we also seen in the simulation, sample size of 10 was also reasonably good. As far as I read this rule of thumb depends mostly on the distribution of the population. some confusing addition to this topic:Furthermore, as we do a statistic test we divide mean differences with estimated standard deviation. With this additional division, sample size starts to matter more apparently.[1] But I haven’t touched this part yet.\r\n\r\n\r\n\r\nHow to calculate SEM in practice?\r\nThe thing that confused me a lot is how to calculate the SEM in practice, especially when you have or don’t have replicates. On the left we measured weights of 4 mice again. According to \\(SEM = \\frac{\\sigma_X}{\\sqrt{N}}\\) we can get SEM. But since we don’t have data from billions of mice that live in the world, we don’t know the population standard deviation (σ). Good thing is we have sample sd (\\(s_X\\)) that we can use as estimate of σ. Sample standard deviation is calculated a little bit differently than population sd:\\[s_X = \\sqrt{\\frac{(X_i - \\bar{X})^2 }{\\sqrt{N - 1}}}\\] At denominator we have N-1 instead of N. This is simply because, when we estimate σ from sample sd we tend to underestimate it. Thus we boost it up. There is also a more logical explanations about this, which I can discuss later.\r\nSo we can calculate the \\(s_X\\) with this formula and calculate the estimate for SEM via \\(SEM = \\frac{s_X}{\\sqrt{N}}\\).\r\nWhat if we have replicates? Then we already calculate mean for each replicate, thus we have more than one mean estimate. Thus one can directly calculate the sd of these means as usual to get the SEM. Which makes sense I think and also according to this resource. THUS, as far as I understood when you have replicates you don’t have to do this \\(\\frac{s_X}{\\sqrt{N}}\\) to get SEM (And I don’t know what to write to the place of \\(s_X\\) to be honest when you have replicates).\r\n\r\n\r\n\r\n\r\n\r\n\r\n1. Irizarry, R. A., & Love, M. I. (2015). Data analysis for the life sciences. https://leanpub.com/dataanalysisforthelifesciences\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-20-random-variables-and-distributions/random-variables-and-distributions_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-11-21T16:55:04+01:00",
    "input_file": "random-variables-and-distributions.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to bblog",
    "description": "hg",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-14T21:05:21+01:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-11-07-post/",
    "title": "Random Variables",
    "description": "first post",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nRandom Variables\r\nNull Distribution\r\n\r\n\r\n\r\n\r\nRandom Variables\r\n\r\n\r\n\r\nLet’s imagine we measured weight of 12 mice. Half treated with a special diet (treatment), other half with usual diet (control). We want to know whether this different diet has an effect on mouse weight or not.\r\n\r\n# A tibble: 12 x 2\r\n# Groups:   condition [2]\r\n   condition weight\r\n   <chr>      <dbl>\r\n 1 control     16.3\r\n 2 control     16.9\r\n 3 control     17.9\r\n 4 control     19.5\r\n 5 control     16.0\r\n 6 control     19.5\r\n 7 treatment   23.4\r\n 8 treatment   21.9\r\n 9 treatment   23.8\r\n10 treatment   22.5\r\n11 treatment   23.6\r\n12 treatment   25.0\r\n\r\nWe then calculate the mean of each group and look at the mean difference of weights.\r\n\\[\r\nμ_t - μ_c = 5.69 \r\n\\]\r\nResults indicate that treated mice are on average 32% heavier than the control ones. So why can’t we just leave it there?\r\nBecause the average values that we get are random variables. If we were to sample and measure another 12 mice for this experiment, we would get a different mean value. In fact these random variables are distribution of values. For example, if we took 10000 samples from the mouse population and measured the mean of these measurements, this would illustrate how this random variable is indeed random and that it varies.\r\n\r\n\r\n\r\nThus we can’t just say: ooh treated mice are 32%* heavier.. We need p-values, Confidence Intervals. There is a variability and we need to take this into account.\r\nNull Distribution\r\nLet’s order 20 mice and divide them randomly into groups of 10 and this time feed them with the same diet. Then again let’s calculate the mean difference of these groups and repeat this 10000 times. When we plot these mean differences we would get a distribution called Null distribution\r\n\r\n\r\n\r\nThis distribution basically shows the variability of random variables (in this case as mean difference), when there is actually no difference. Another thing it shows is that how probable it is to observe a certain value. In this case, how probable it is to observe a mean difference value of x when there is no difference in compared populations.\r\nSuppose that the mean difference of weight between treated mice and control mice was 1.55. If we look at our null distribution graph and calculate the percentage of the area that is higher than 1.55, we would see that it is approximately 6% of the plot. This means that, if there was no difference between groups; 6% of the time, we would get a value of 1.55 or higher. And this would be our p-value\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-07-post/post_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-11-14T22:42:51+01:00",
    "input_file": "post.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
