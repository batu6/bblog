[
  {
    "path": "posts/2021-11-27-inference/",
    "title": "Random variable Characteristics, Statistical Inference & t-test",
    "description": "Here I talk about some characteristics of random variables, then we see how these characteristics are applied in statistical testing and t-test makes an appearance.",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nExample\r\nIn practice\r\nIllustration\r\n\r\n\r\n\r\n\r\nIt is time we talk about some math(dört işlem) with random variables. As CLT indicates (see the post), we now that when sample size is large the means are distributed randomly. To illustrate the math with random variables more easily, let’s consider we have the following measurements and let’s assume that they are normally distributed: \\(X = (10cm, 20cm, 30cm)\\). It has a mean \\(\\bar{X}\\) of 20 and sd \\(S_x\\) of ~8.2.\r\nIf we add or subtract a constant value from our random variable, the new mean \\(\\bar{X}{new}\\) becomes \\(\\bar{X} + constant\\) and our sd \\(S_x\\) does not change since the average distance of the variables to the mean is still the same.\r\nIf we multiply our random variable \\(X\\) with a constant, for example we converted our cm measurements to mm, the new mean \\(\\bar{X}{new}\\) becomes \\(\\bar{X} * constant\\) and our sd \\(S_x\\) becomes also \\(S_{new} = S_x * constant\\) (now the spread of our data points increased).\r\n\r\n\r\n\r\nFigure 1: Some math with random variables\r\n\r\n\r\n\r\nSo far it is clear I believe. So what if we sum or multiply our random variable \\(X\\) with another random variable \\(Y\\). This type of a calculation we encounter more. Generally we are interested in mean differences of two populations for example. Then we subtract these means right? Please note that, as our random variables, their mean and sd are also random variables. So let’s see how the things will work out!\r\nThe mean:\r\nAs previously, when we sum or subtract the two random variables their mean also gets summed up or subtracted. So: \\(mean(X + Y) = \\bar{X} + \\bar{Y}\\) or \\(mean(X - Y) = \\bar{X} - \\bar{Y}\\). This was easy.\r\nThe Sd:\r\nThis time we need to assume a few things and we will also work with variance. Variance is square of the sd \\(S_x^2\\). The reason we will use variance instead of sd is that simply it makes the math much more simple.\r\nIf we skip the math we get this equation:\r\n\\[\r\nVar(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)\r\n\\]\r\nHere we see a new fellow: Covariance (but we will get rid of him in a sec şşşşşş). Which is:\r\n\\[\r\nCov(X,Y) = Cor(X,Y)*\\sigma_X*\\sigma_Y\r\n\\]\r\nHere we see a more familiar face Correlation. It has come to tell us our first assumption, our random variables should not correlate with each other. Which means \\(Cor(X,Y) = 0\\), which makes \\(Cov(X,Y) = 0\\), which results in:\r\n\\[\r\nVar(X+Y) = Var(X) + Var(Y)\r\n\\] This tells us that variance of the sum of \\(X\\) and \\(Y\\) is equal to sum of their variances. GIVEN they are not correlated.\r\nExample\r\n\r\n\r\nset.seed(1)\r\n\r\n# Almost not correlated random variables:\r\nx <- runif(n = 100, min = 10,max = 50)\r\ny <- runif(n = 100, min = 60,max = 100)\r\n\r\n\r\n# Correlated random variables:\r\na <- 1:100\r\nb <- seq(50,200, length.out=100)\r\n\r\n\r\n\r\nWhen they are not correlated:\r\nCor(X,Y) = 0.02\r\nalmost no correlation.\r\nvar(x) + var(y) = 232.78\r\nvar(x + y) = 236.74\r\nclose enough!\r\nWhen they are correlated:\r\nCor(a,b) = 1\r\nfull correlation\r\nvar(a) + var(b) = 2773.87\r\nvar(a + b) = 5324.37\r\nAs you can see when our random variables were correlated our variance estimate was way off.\r\nIn practice\r\nSo in practice we compare the mean differences to a distribution. In the first postthe first post we had calculated the null distribution by taking unreal amount of samples from the population. Now we will do it using statistics.\r\nTo calculate a test statistic we divide the random variable with its standard deviation. This, in theory gives us a normal distribution with mean = 0 and sd = 11 (given high sample size)2. This also allows us to standardize the data. Even if the original data values are at the range of thousands, we standardize it to a standard distribution.\r\n\\[\r\n\\frac{\\bar{X}-\\bar{Y}}{sd_{XY}}\r\n\\]\r\n1. Here note that both numerator and denominator are random variables. 2. As we have seen above if we subtract two random variables, their mean also gets subtracted, hence numerator. (we are using sample means as an estimate of the real population means.) 3. We also saw that when we subtracted two random variables the new variance is equal to the sum of the individual variances. Thus \\(sd_{XY}\\) will be equal to \\(\\sqrt{\\frac{s_X^2}{M} + \\frac{s_Y^2}{N}}\\) where M and N are sample sizes of random variables X and Y. 4. If sample size is high, according to CLT our random variables will be also normal. Thus the parameters at the numerator and denominator will be also normal3. 5. The issue is division of two normal values may not be normal. This is due to the fact that we can underestimate the sd4 (Especially when the sample size is low). Thus our probability of observing bigger test statistic values increases. (see the Illustration section at the bottom)\r\nThis change in probabilities creates a new distribution, that we know as Student’s t distribution. More on the story\r\nThis is the final version of our statistic. Mean difference of the random variables divided by its standard deviation5.6\r\n\\[\r\n\\frac{\\bar{X}-\\bar{Y}}{\\sqrt{\\frac{s_X^2}{M} + \\frac{s_Y^2}{N}}}\r\n\\]\r\nWilliam Sealy Gosset\r\n\r\n\r\n\r\nFigure 2: https://en.wikipedia.org/wiki/William_Sealy_Gosset\r\n\r\n\r\n\r\nCLT is dependent on high sample size. Our estimate of variances are accurate when the sample size is high and thus the test statistic follows a standard normal distribution. However it does not account for the underestimation of variance when our sample size is not that high. t-distribution looks similar to normal distribution formula but it also comes with an additional flavor called degrees of freedom (df), that accounts for this. Where \\(df = n - 1\\)\r\n\r\n\r\n\r\nFigure 3: t-distribution with different degrees of freedom, code from: https://stackoverflow.com/questions/49236741/plot-student-s-t-distribution-with-degrees-of-freedom\r\n\r\n\r\n\r\nFor the small sample sizes, distribution becomes more wide(higher tails, red line) and it accounts for the higher probability of observing extremes7. As the sample size increases t-distribution also starts to resemble the normal distribution.\r\nIllustration\r\nHere I will directly use the code from the book “Data Analysis for the Life Sciences” to show it.[1] Here we have a population of mice, half treated with a control diet other half treated with different diet and we compare their weights. From each group we take samples and calculate the mean difference of our samples. We repeat this 10000 times. We will compare different sample sizes: 3, 12, 25 and 50.\r\n\r\n[1] 2.375517\r\n\r\n\r\nFigure 4: Mean differences\r\n\r\n\r\n\r\nHere we see with quantile plots, how our mean differences compare to a normal distribution.\\(\\bar{X}-\\bar{Y}\\) We can see that even at sample size of 3 our differences are relatively normal. This is mainly due to the fact that weight is a relatively normally distributed factor. Since the population is normal our samples are also normal. Thus we see a good fit even with low sample size.\r\nHowever as I talked about it above, we also divide mean difference of random variables with its sd. \\(\\frac{\\bar{X}-\\bar{Y}}{\\sqrt{\\frac{s_X^2}{M} + \\frac{s_Y^2}{N}}}\\) For the next plot, we divide mean differences with its sd. We repeat this 10000 times. Now we see that ratio of these two random variables indeed deviate from normal at normal distribution at low sample size. At even sample size of 12 we see some deviation at the edges. At the higher sample sizes it again fit to normal distribution.\r\n\r\n\r\n\r\nFigure 5: Mean differences divided by standard deviations\r\n\r\n\r\n\r\n\r\n\r\n\r\n1. Irizarry, R. A., & Love, M. I. (2015). Data analysis for the life sciences. https://leanpub.com/dataanalysisforthelifesciences\r\n\r\n\r\n(standard normal distribution)↩︎\r\nCLT↩︎\r\nIn the first post when we plotted the mean differences we saw that they were normally distributed↩︎\r\n(denominator)↩︎\r\naka SEM↩︎\r\nSince in this example we are comparing two random variables, if we do a t-test it would be called two sample t-test↩︎\r\nProbability of observing extremes increases, because we tend to underestimate the true variance at low sample size, as I discussed above↩︎\r\n",
    "preview": "posts/2021-11-27-inference/inference_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-11-28T14:08:57+01:00",
    "input_file": "inference.knit.md",
    "preview_width": 2304,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-11-20-random-variables-and-distributions/",
    "title": "Random Variables, Distributions and CLT",
    "description": "We talk about CLT, sample size and SEM",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntro\r\nCentral Limit Theorem\r\nWhat is SEM actually?\r\nSimulation with different sample sizes\r\nSample size vs Replicates\r\nHow to calculate SEM in practice?\r\n\r\n\r\n\r\n\r\n\r\nIntro\r\nIn the last post we bought thousands of mice to get the null distribution of the mouse weights. But we cannot do that every time can we? At this point statistical inference comes to the rescue, which helps us to infer the probability of observing certain outcome, with just from small amount of samples. It can do its magic because sometimes the values we have can be approximated with a certain distribution. Our mean weight differences, when plotted follows a normal distribution for example (red line). Thus if such a distribution is applicable to our values one can use that distribution to get an approximate probability of certain outcome.\r\n\r\n\r\n\r\nFigure 1: Null distribution from last post\r\n\r\n\r\n\r\nOk, let’s use normal distribution, but how can we adjust this distribution to fit our data? In order to define a normal distribution we need two values: mean(μ) and standard deviation(σ) of our population. (as one can see from the scary normal distribution formula)\r\n\\[\r\n\\mbox{f}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left( \\frac{-(x-\\mu)^2}{2 \\sigma^2} \\right)}\r\n\\]\r\nCentral Limit Theorem\r\n\r\n\r\n\r\nFigure 2: Population and Sample Parameters\r\n\r\n\r\n\r\nSo we need to estimate our population mean \\(μ_X\\) and population sd \\(\\sigma_X\\).\r\nwhich we can do with our sample mean \\(\\bar{X}\\) and sample sd \\(s_X\\), with the help of one and only Central Limit Theorem(CLT).\r\nCLT says that when the sample size (N) is large, the average of these random samples \\(\\bar{X}\\) follow a normal distribution that has the mean of the population \\(μ_X\\) and has a sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\).\r\nIn another words, imagine that we sample 30 people randomly and record the mean height. Then repeat this couple of times so that we have several means. Distribution of these sampling means would follow a normal distribution with the mean \\(μ_X\\) and with sd \\(\\frac{\\sigma_X}{\\sqrt{30}}\\).\r\nNote that \\(\\frac{\\sigma_X}{\\sqrt{N}}\\) is also famously known as Standard Error of the Mean (SEM).\r\nWhat is SEM actually?\r\nStandard Error of the Mean.. What a confusing name… But apparently this was not the case before. This is actually it is nick name it seems. Its full name is “The estimated standard deviation of the sampling distribution of x-bar” according to this discussion. Which is longer but more explanatory. As it suggests, SEM is just the standard deviation of the means(\\(\\bar{X}\\)) of your samples. So you took four measurements and calculated mean of it, repeated this 10 times. You get 10 means, standard deviation of these means is SEM.\r\nOna sadece iş arkadaşları SEM der.\r\n\r\n\r\n\r\nFigure 3: Figure from: https://www.haberler.com/fotogaleri/yilan-hikayesi-nin-memoli-sinin-son-hali/\r\n\r\n\r\n\r\nSimulation with different sample sizes\r\nHere I simulated different populations with different characteristics, each with size of 200.000, mean of μ = ~130 and sd of σ = ~34. On the left we see the different populations (bimodal, normal, uniform). I sampled from these populations with different sample sizes (n=2, 10, 30, 50), calculated the mean and repeated this process 100 times (green histograms). I also plotted two normal distributions and means. Black lines show the calculations done by using population parameters; normal distribution with sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\) and mean: \\(μ_P\\). Red lines show the empirical estimate from our sampling means; normal distribution with sd \\(s_X\\) and mean \\(\\bar{X}\\).\r\nCLT\r\nNe olursan ol yine gel\r\ndiyen Mevlana,\r\nBen insanın değerini bölemem\r\nDoğu-batı, gavur-müslüm bir bana!\r\ndiyen Aşık Mahzuni Şerif gibi.\r\nHere one can see that actually even with sample size of 2 we get a quite accurate estimate of the mean and sd of the means(SEM). Although the actual mean calculations(histograms) are not completely normally distributed, it still resembles a normal distribution. As we increase the sample size we see that the distribution of the means get much more similar to the suggested normal distribution by CLT (with the mean of the population, and sd of \\(\\frac{\\sigma_X}{\\sqrt{N}}\\)).\r\nAnother thing that we can notice is that SEM decreases as we increase the sample size. We get much more accurate estimate of the real population mean with high N.\r\nLastly we have seen that CLT did not care about the population distribution too much. In the end we got normally distributed means.\r\n\r\n\r\n\r\nFigure 4: Sample size differences illustration\r\n\r\n\r\n\r\nI would like to touch on a few points which always confused me. I think I finally got it. :)\r\nSample size vs Replicates\r\nI think sometimes it gets quite confusing to understand what the sample size is, at least for me.. If we look at the following figure, on the left side I took 4 samples. Let’s say I took 4 mice and measured their weight and that is it. In this case I have a sample size of 4 and I only performed 1 trial/replicate. On the right side I took 4 mice again but repeated this 5 times in total. Here my sample size is 4 again and I just replicated it 5 times.\r\nOn the right bottom I also illustrated that as we do more replicates we would expect these sampling means from each replicate to form a normal distribution. Higher our sample size is, more it resembles a normal distribution according to CLT. In statistics they say minimum sample size should be 30 as a rule of thumb, but as we also seen in the simulation, sample size of 10 was also reasonably good. As far as I read this rule of thumb depends mostly on the distribution of the population. Some confusing addition to this topic: Furthermore, as we do a statistic test we divide mean differences with estimated standard deviation. With this additional division, sample size starts to matter more apparently.[1] See the next post about this.\r\n\r\n\r\n\r\nFigure 5: Sampling\r\n\r\n\r\n\r\nHow to calculate SEM in practice?\r\nThe thing that confused me a lot is how to calculate the SEM in practice, especially when you have or don’t have replicates. In the next figure, on the left we measured weights of 4 mice again. According to \\(SEM = \\frac{\\sigma_X}{\\sqrt{N}}\\) we can get SEM. But since we don’t have data from billions of mice that live in the world, we don’t know the population standard deviation (σ). Good thing is we have sample sd (\\(s_X\\)) that we can use as estimate of σ. Sample standard deviation is calculated a little bit differently than population sd:\\[s_X = \\sqrt{\\frac{(X_i - \\bar{X})^2 }{N - 1}}\\] We use sample mean and at denominator we have N-1 instead of N. This is simply because, when we estimate σ from sample sd we tend to underestimate it. Thus we boost it up by dividing with a smaller number. There is also a more logical explanations about this, which I can discuss later.\r\nSo we can calculate the \\(s_X\\) with this formula and calculate the estimate for SEM via \\(SEM = \\frac{s_X}{\\sqrt{N}}\\).\r\nWhat if we have replicates? Then we already calculate mean for each replicate, thus we have more than one mean estimate. Thus one can directly calculate the sd of these means as usual to get the SEM. Which makes sense I think and also according to this resource. THUS, as far as I understood when you have replicates you don’t have to do this \\(\\frac{s_X}{\\sqrt{N}}\\) to get SEM (And I don’t know what to write to the place of \\(s_X\\) to be honest when you have replicates).\r\n\r\n\r\n\r\nFigure 6: Calculation of SEM\r\n\r\n\r\n\r\n\r\n\r\n\r\n1. Irizarry, R. A., & Love, M. I. (2015). Data analysis for the life sciences. https://leanpub.com/dataanalysisforthelifesciences\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-20-random-variables-and-distributions/images/Ps_mean.png",
    "last_modified": "2021-11-28T14:06:05+01:00",
    "input_file": "random-variables-and-distributions.knit.md",
    "preview_width": 1022,
    "preview_height": 781
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to bblog",
    "description": "hg",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-14T21:05:21+01:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-11-07-post/",
    "title": "Random Variables",
    "description": "first post",
    "author": [
      {
        "name": "Batuhan Akçabozan",
        "url": {}
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nRandom Variables\r\nNull Distribution\r\n\r\n\r\n\r\n\r\nRandom Variables\r\n\r\n\r\n\r\nLet’s imagine we measured weight of 12 mice. Half treated with a special diet (treatment), other half with usual diet (control). We want to know whether this different diet has an effect on mouse weight or not.\r\n\r\n# A tibble: 12 x 2\r\n# Groups:   condition [2]\r\n   condition weight\r\n   <chr>      <dbl>\r\n 1 control     16.3\r\n 2 control     16.9\r\n 3 control     17.9\r\n 4 control     19.5\r\n 5 control     16.0\r\n 6 control     19.5\r\n 7 treatment   23.4\r\n 8 treatment   21.9\r\n 9 treatment   23.8\r\n10 treatment   22.5\r\n11 treatment   23.6\r\n12 treatment   25.0\r\n\r\nWe then calculate the mean of each group and look at the mean difference of weights.\r\n\\[\r\nμ_t - μ_c = 5.69 \r\n\\]\r\nResults indicate that treated mice are on average 32% heavier than the control ones. So why can’t we just leave it there?\r\nBecause the average values that we get are random variables. If we were to sample and measure another 12 mice for this experiment, we would get a different mean value. In fact these random variables are distribution of values. For example, if we took 10000 samples from the mouse population and measured the mean of these measurements, this would illustrate how this random variable is indeed random and that it varies.\r\n\r\n\r\n\r\nThus we can’t just say: ooh treated mice are 32%* heavier.. We need p-values, Confidence Intervals. There is a variability and we need to take this into account.\r\nNull Distribution\r\nLet’s order 20 mice and divide them randomly into groups of 10 and this time feed them with the same diet. Then again let’s calculate the mean difference of these groups and repeat this 10000 times. When we plot these mean differences we would get a distribution called Null distribution\r\n\r\n\r\n\r\nThis distribution basically shows the variability of random variables (in this case as mean difference), when there is actually no difference. Another thing it shows is that how probable it is to observe a certain value. In this case, how probable it is to observe a mean difference value of x when there is no difference in compared populations.\r\nSuppose that the mean difference of weight between treated mice and control mice was 1.55. If we look at our null distribution graph and calculate the percentage of the area that is higher than 1.55, we would see that it is approximately 6% of the plot. This means that, if there was no difference between groups; 6% of the time, we would get a value of 1.55 or higher. And this would be our p-value\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-07-post/post_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-11-14T22:42:51+01:00",
    "input_file": "post.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
